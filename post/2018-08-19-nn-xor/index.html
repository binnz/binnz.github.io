<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Neural Network Architecture and Back-Propagation - California Dreaming</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
  
  <meta name="description" content="The Universal Approximation Theorem states that a 2-layer network can approximate any function, given a complex enough architecture.  That&rsquo;s why we will create a neural network with two neurons in the hidden layer and we will later show how this can model the XOR function.
In this experiment, we will need to understand and write a simple neural network with backpropagation for “XOR” using only numpy and other python standard library.
The code here will allow the user to specify any number of layers and neurons in each layer.  In addition, we are going to use the logistic function as the activity function for this network.">
  
  <meta itemprop="name" content="Neural Network Architecture and Back-Propagation - California Dreaming">
  <meta itemprop="description" content="The Universal Approximation Theorem states that a 2-layer network can approximate any function, given a complex enough architecture.  That&rsquo;s why we will create a neural network with two neurons in the hidden layer and we will later show how this can model the XOR function.
In this experiment, we will need to understand and write a simple neural network with backpropagation for “XOR” using only numpy and other python standard library.
The code here will allow the user to specify any number of layers and neurons in each layer.  In addition, we are going to use the logistic function as the activity function for this network.">
  <meta itemprop="image" content="https://binnz.github.io/img/author.jpg">
  
  
  <meta name="twitter:description" content="">
  
  <link rel="shortcut icon" href="https://binnz.github.io/img/favicon.ico"/>
  <link rel="apple-touch-icon" href="https://binnz.github.io/apple-touch-icon.png" />
  <link rel="apple-touch-icon-precomposed" href="https://binnz.github.io/apple-touch-icon.png" />
  <link rel="stylesheet" href="https://binnz.github.io/highlight/styles/github.css">
  <script src="https://binnz.github.io/highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  
  <link rel="stylesheet" href="https://binnz.github.io/font/hack/css/hack.min.css">
  <link rel="stylesheet" href="https://binnz.github.io/css/style.css">
</head>

<body>
  <header>
    <div>
  <div style="height:150px; width:100%; clear:both;"></div>
  <div id="textlogo">
    <h1 class="site-name"><a href="https://binnz.github.io/" title="California Dreaming">California Dreaming</a></h1>
    <h2 class="blog-motto"></h2>
  </div>
  <div class="navbar"><a class="navbutton navmobile" href="#" title="menu"></a></div>
			<div style="height:50px; width:100%; clear:both;"></div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="menu">
			</a><div style="height:50px; width:100%; clear:both;"></div></div>
  <nav class="animated">
    <ul>
      
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/post/">Archives</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      
      <li>
        <form class="search" method="get" action="https://www.google.com/search">
          <div>
            <input type="text" id="search" name="q" placeholder='Search'>
          </div>
        </form>
      </li>
    </ul>
  </nav>
</div>

  </header>
  <div id="container">
    <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody">
    <header class="article-info clearfix">
  <h1 itemprop="name">
      <a href="https://binnz.github.io/post/2018-08-19-nn-xor/" title="Neural Network Architecture and Back-Propagation" itemprop="url">Neural Network Architecture and Back-Propagation</a>
  </h1>
  <p class="article-author">By
    
      <a href="" title="">you</a>
    
  </p>
  <p class="article-time">
    <time datetime="2018-08-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">2018-08-19</time>
  </p>
</header>

	<div class="article-content">
    
    <p>The Universal Approximation Theorem states that a 2-layer network can approximate any function, given a complex enough architecture.  That&rsquo;s why we will create a neural network with two neurons in the hidden layer and we will later show how this can model the XOR function.</p>
<p>In this experiment, we will need to understand and write a simple neural network with backpropagation for “XOR” using only <code>numpy</code> and other python standard library.</p>
<p>The code here will allow the user to specify any number of layers and neurons in each layer.  In addition, we are going to use the logistic function as the activity function for this network.</p>
<h2 id="i-experiment-setups">I. Experiment Setups</h2>
<h3 id="a-sigmoid-function">A. sigmoid function</h3>
<p>In this practice, sigmoid function will be used for activation.   According to Wikipedia, a sigmoid function is a mathematical function having a characteristic &ldquo;S&rdquo;-shaped curve or sigmoid curve.</p>
<p>Often, sigmoid function refers to the special case of the logistic function shown in the figure above and defined by the formula</p>
<p>$$
g(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{e^x+1}
$$</p>
<p>which can be written in python code with <code>numpy</code> library as follows</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><p>Then, to take the derivative in the process of back propagation, we need to do differentiation of logistic function.</p>
<p>Suppose the output of a neuron (after activation) is $y = g(x) = (1+e^{-x})^{-1}$ where $x$ is the net input to this neuron, then the differentiation of logistic function is</p>
<p>$$
g'(x) =-(1+\exp(-x))^{-2}\exp(-x)(-1)
\<br>
=g(x)\frac{\exp(-x)}{1+\exp(-x)}
=g(x)\frac{1+\exp(-x)-1}{1+\exp(-x)}
\<br>
=g(x)(1-g(x))
$$</p>
<p>So when we take the partial derivative $\partial y / \partial x=y(1-y)$, we can use the following python function</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">y</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="b-neural-network">B. neural network</h3>
<p>We devised a class named <code>NeuralNetwork</code> that is capable of training a &ldquo;XOR&rdquo; function.   The <code>NeuralNetwork</code> consists of the following 3 parts:</p>
<ol>
<li>initialization</li>
<li>fit</li>
<li>predict</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">:</span>

    <span class="c1">#########</span>
    <span class="c1"># parameters</span>
    <span class="c1"># ----------</span>
    <span class="c1"># self:      the class object itself</span>
    <span class="c1"># net_arch:  consists of a list of integers, indicating</span>
    <span class="c1">#            the number of neurons in each layer, </span>
    <span class="c1">#            i.e. [2,2,1] (two neurons for the input layer, </span>
    <span class="c1">#                          two neurons for the first and the only hidden layer, </span>
    <span class="c1">#                          and one neuron for the output layer)</span>
    <span class="c1">#########</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net_arch</span><span class="p">):</span>
        <span class="c1"># Initialized the weights, making sure we also initialize the weights</span>
        <span class="c1"># for the biases that we will add later.</span>
        <span class="c1"># Afterwards, we do random initialization with range of weight values (-1,1)</span>

    <span class="k">def</span> <span class="nf">_forward_prop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># feed-forward</span>
    
    <span class="k">def</span> <span class="nf">_back_prop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="c1"># adjust the weights using the backpropagation rules</span>

    <span class="c1">#########</span>
    <span class="c1"># parameters</span>
    <span class="c1"># ----------</span>
    <span class="c1"># self:    the class object itself</span>
    <span class="c1"># data:    the set of all possible pairs of booleans True or False indicated by </span>
    <span class="c1">#          the integers 1 or 0</span>
    <span class="c1"># labels:  the result of the logical operation &#39;xor&#39; on each of those input pairs</span>
    <span class="c1">#########</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="c1"># Add bias units to the input layer</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="c1"># Set up our feed-forward propagation</span>
            <span class="c1"># And then do our back-propagation of the error to adjust the weights</span>

    <span class="c1">#########</span>
    <span class="c1"># parameters</span>
    <span class="c1"># ----------</span>
    <span class="c1"># self:   the class object itself</span>
    <span class="c1"># X:      the input data array</span>
    <span class="c1">#########</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># Do prediction with the given data X and the pre-trained weights</span>
</code></pre></td></tr></table>
</div>
</div><p>(for full code implementation, please see the <a href="#iv-appendix">Appendix</a>)</p>
<p>In the initialization part, we create a <strong>list of arrays</strong> for the weights.   That is, given $k$ layers (the $1^{th}$ layer is the input layer and the $k^{th}$ layer is the output layer) and $n_k$ units in the $k^{th}$ layer, we have</p>
<p>$$
self.weights = [\Theta^{(1)}~\Theta^{(3)}~&hellip;~\Theta^{(k-1)}]
\<br>
where~\Theta^{(j)}=\begin{bmatrix}
\Theta_{1,1}^{(j)} &amp; \Theta_{1,2}^{(j)} &amp; \Theta_{1,3}^{(j)} &amp; \dots  &amp; \Theta_{1,n_{j+1}}^{(j)} \<br>
\Theta_{2,1}^{(j)} &amp; \Theta_{2,2}^{(j)} &amp; \Theta_{2,3}^{(j)} &amp; \dots  &amp; \Theta_{2,n_{j+1}}^{(j)} \<br>
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>
\Theta_{(n_j+1),1}^{(j)} &amp; \Theta_{(n_j+1),2}^{(j)} &amp; \Theta_{(n_j+1),3}^{(j)} &amp; \dots  &amp; \Theta_{(n_j+1),n_{j+1}}^{(j)}
\end{bmatrix}
$$</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Random initialization with range of weight values (-1,1)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">net_arch</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">net_arch</span><span class="p">[</span><span class="n">layer</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>Note that a bias unit is added to each hidden layer and a &ldquo;1&rdquo; will be added to the input layer.   That&rsquo;s why the dimension of weight matrix is $(n_j+1) \times n_{j+1}$ instead of $n_j \times n_{j+1}$.</p>
<p>The fit part will train our network.   For each epoch, we sample a training data and then do <strong>forward propagation</strong> and <strong>back propagation</strong> with this input.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># We will now go ahead and set up our feed-forward propagation:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">Z</span><span class="p">[</span><span class="n">sample</span><span class="p">]]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_prop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Now we do our back-propagation of the error to adjust the weights:</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">sample</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_back_prop</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>Forward propagation propagates the sampled input data forward through the network to generate the output value.</p>
<p>According to the generated output value, back propagation calculates the cost (error term) and do the propagation of the output activations back through the network using the training pattern target in order to generate the deltas (the difference between the targeted and actual output values) of all output and hidden neurons.   With these deltas, we can get the gradients of the weights and use these gradients to update the original weights.</p>
<h3 id="c-backpropagation">C. backpropagation</h3>
<p><img src="https://i.imgur.com/WsePKKW.png" alt=""></p>
<p>Use the neural network shown in <strong>Figure above</strong> as an example, the final output of the model would be</p>
<p>$$
h_{\Theta}(x) = a_1^{(3)} = g(z_1^{(3)}) = g(\Theta_{0,1}^{(2)}a_0^{(2)}+\Theta_{1,1}^{(2)}a_1^{(2)}+\Theta_{2,1}^{(2)}a_2^{(2)})
$$</p>
<ul>
<li>$\Theta^{(j)}$ is the matrix of weights mapping from layer $j$ to layer $(j+1)$</li>
<li>$a_i^{(j)}$ is the activation of unit $i$ in layer $j$</li>
<li>$z_i^{(j)}$ is the net input to the unit $i$ in layer $j$</li>
<li>$g$ is sigmoid function that refers to the special case of the logistic function</li>
<li>$x$ is the input vector $[x_0~x_1~x_2]^T$.</li>
</ul>
<p>To update the weights with <em>gradient descent</em> method, we need to calculate the gradients.   Ultimately, this means computing the partial derivatives $\partial err / \partial a_1^{(3)}$ given the error term $E_{total}$ defined as $E_{total} = (1/2)(y - a_1^{(3)})^2$, which is the loss between the actual label $y$ and the prediction $a_1^{(3)}$.</p>
<p>Note that with chain rule, the partial derivative of $E_{total}$ with respect to $\Theta_{2,1}^{(2)}$ is only related to the error term and the output values $a_2^{(2)}$ and $a_1^{(3)}$.</p>
<p>$$
\frac{\partial~E_{total}}{\partial~\Theta_{2,1}^{(2)}} = \frac{\partial~E_{total}}{\partial~a_1^{(3)}} \times \frac{\partial~a_1^{(3)}}{\partial~\Theta_{2,1}^{(2)}}
= \frac{\partial~E_{total}}{\partial~a_1^{(3)}} \times \Bigg(\frac{\partial~g(z_1^{(3)})}{\partial~z_1^{(3)}} \times \frac{\partial~z_1^{(3)}}{\partial~\Theta_{2,1}^{(2)}}\Bigg)
\<br>
= \Bigg(-(y - a_1^{(3)})\Bigg) \times \Bigg(\Big(a_1^{(3)} * (1-a_1^{(3)})\Big) \times a_2^{(2)}\Bigg)
$$</p>
<p>Furthermore, the partial derivative of $E_{total}$ with respect to $\Theta_{2,1}^{(1)}$ can be calculated with the same regards as follows.</p>
<p>$$
\frac{\partial~E_{total}}{\partial~\Theta_{2,1}^{(1)}} = \frac{\partial~E_{total}}{\partial~a_2^{(2)}} \times \frac{\partial~a_2^{(2)}}{\partial~\Theta_{2,1}^{(1)}}
= \frac{\partial~E_{total}}{\partial~a_2^{(2)}} \times \Bigg(\frac{\partial~g(z_2^{(2)})}{\partial~z_2^{(2)}} \times \frac{\partial~z_2^{(2)}}{\partial~\Theta_{2,1}^{(1)}}\Bigg)
\<br>
where~\frac{\partial~E_{total}}{\partial~a_i^{(j)}}=\sum_{k}\frac{\partial~E_{a_{k}^{(j+1)}}}{\partial~a_i^{(j)}}=\sum_{k} \frac{\partial~E_{a_{k}^{(j+1)}}}{\partial~z_k^{(j+1)}} \times \Theta_{k,i}^{(j)}
$$</p>
<p>In conclusion, the back propagation process can be divided into 2 steps:</p>
<p><strong><u>Step 1.</u> Generate the deltas (the difference between the targeted and actual output values) of all output and hidden neurons</strong></p>
<p>First, we need to calculate the partial derivative of the total error with respect to the net input values of the neuron(s) in the output layer.</p>
<p>Recall that we have calculated the partial derivative of the total error $E_{total}$ with respect to $z_1^{(3)}$, which is the net input to the neuron in the output layer in the case we discuss above.</p>
<p>$$
E_{z_1^{(3)}} = \frac{\partial~E_{total}}{\partial~z_1^{(3)}} = \frac{\partial~E_{total}}{\partial~a_1^{(3)}} \times \frac{\partial~g(z_1^{(3)})}{\partial~z_1^{(3)}}
= \Bigg(-(target - a_1^{(3)})\Bigg) \times \Bigg(a_1^{(3)} * (1-a_1^{(3)})\Bigg)
$$</p>
<p>This can be written in a general form as</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">delta_vec</span> <span class="o">=</span> <span class="p">[(</span><span class="n">target</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">activity_derivative</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
</code></pre></td></tr></table>
</div>
</div><p>where $y[j] = [a_{0}^{(j)}~a_{1}^{(j)}~&hellip;]$ is a vector representing the output values of layer $j$ and the <code>delta</code> we compute here is actually the **negative** gradient.</p>
<p>Afterwards, we calculate the deltas for neurons in the remaining layers.</p>
<p>For the remaining layers, given $\Theta_{pq}^{(j)}$ as the weight maps from the $p^{th}$ unit of layer $j$ to the $q^{th}$ unit of layer $(j+1)$, we have</p>
<p>$$
E_{z_i^{(j)}} = \frac{\partial~E_{total}}{\partial~z_{i}^{(j)}} = \frac{\partial~E_{total}}{\partial~a_{i}^{(j)}} \times \frac{\partial~a_{i}^{(j)}}{\partial~z_{i}^{(j)}}
= \Bigg(\sum_{k}\frac{\partial~E_{a_{k}^{(j+1)}}}{\partial~a_i^{(j)}} \Bigg) \times \frac{\partial~a_{i}^{(j)}}{\partial~z_{i}^{(j)}}
\<br>
=\Bigg(\sum_{k} \frac{\partial~E_{a_{k}^{(j+1)}}}{\partial~z_k^{(j+1)}} \times \frac{\partial~z_k^{(j+1)}}{\partial~a_{i}^{(j)}} \Bigg) \times \frac{\partial~a_{i}^{(j)}}{\partial~z_{i}^{(j)}}
=\Bigg(\sum_{k} E_{z_k^{(j+1)}} \times \Theta_{i,k}^{(j)} \Bigg) \times \frac{\partial~a_{i}^{(j)}}{\partial~z_{i}^{(j)}}
$$</p>
<p>As a result, when we consider the matrix representation of weights,</p>
<p>$$
self.weights[j]=\Theta^{(j)}=\begin{bmatrix}
\Theta_{1,1}^{(j)} &amp; \Theta_{1,2}^{(j)} &amp; \Theta_{1,3}^{(j)} &amp; \dots  &amp; \Theta_{1,n_{j+1}}^{(j)} \<br>
\Theta_{2,1}^{(j)} &amp; \Theta_{2,2}^{(j)} &amp; \Theta_{2,3}^{(j)} &amp; \dots  &amp; \Theta_{2,n_{j+1}}^{(j)} \<br>
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>
\Theta_{(n_j+1),1}^{(j)} &amp; \Theta_{(n_j+1),2}^{(j)} &amp; \Theta_{(n_j+1),3}^{(j)} &amp; \dots  &amp; \Theta_{(n_j+1),n_{j+1}}^{(j)}
\end{bmatrix}
$$</p>
<p>we can calculate the gradient of weights layer-by-layer from the last hidden layer to the input layer with the code below.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># we need to begin from the back, from the next to last layer</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">delta_vec</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>   <span class="c1"># Line 3</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">error</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">activity_derivative</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">:])</span>   <span class="c1"># Line 4</span>
    <span class="n">delta_vec</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>

<span class="c1"># Now we need to set the values from back to front</span>
<span class="n">delta_vec</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p>Note that for a certain layer $j$, the inner product generated by Line 3 of the code above represents</p>
<p>$$
\bigg[\frac{\partial~E_{total}}{\partial~z_{2}^{(j)}}~\frac{\partial~E_{total}}{\partial~z_{3}^{(j)}}~\frac{\partial~E_{total}}{\partial~z_{4}^{(j)}}~&hellip;\frac{\partial~E_{total}}{\partial~z_{(n_j+1)}^{(j)}}\bigg]
$$</p>
<p>$$
=[E_{z_2^{(j+1)}}~E_{z_2^{(j+1)}}~E_{z_3^{(j+1)}}~&hellip;~E_{z_{(n_j+1)}^{(j+1)}}] \cdot \begin{bmatrix}
\Theta_{2,1}^{(j)} &amp; \Theta_{2,2}^{(j)} &amp; \Theta_{2,3}^{(j)} &amp; \dots  &amp; \Theta_{2,n_{j+1}}^{(j)} \<br>
\Theta_{3,1}^{(j)} &amp; \Theta_{3,2}^{(j)} &amp; \Theta_{3,3}^{(j)} &amp; \dots  &amp; \Theta_{3,n_{j+1}}^{(j)} \<br>
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>
\Theta_{(n_j+1),1}^{(j)} &amp; \Theta_{(n_j+1),2}^{(j)} &amp; \Theta_{(n_j+1),3}^{(j)} &amp; \dots  &amp; \Theta_{(n_j+1),n_{j+1}}^{(j)}
\end{bmatrix}
$$</p>
<p>And in Line 4 we generate <code>delta_vec[j]</code> with</p>
<p>$$
delta_vec[j]
=\bigg[E_{z_2^{(j)}}~E_{z_3^{(j)}}~E_{z_4^{(j)}}~&hellip;~E_{z_{n_j+1}^{(j)}}\bigg]
\<br>
= \bigg[\frac{\partial~E_{total}}{\partial~z_{2}^{(j)}}~\frac{\partial~E_{total}}{\partial~z_{3}^{(j)}}~\frac{\partial~E_{total}}{\partial~z_{4}^{(j)}}~&hellip;\frac{\partial~E_{total}}{\partial~z_{(n_j+1)}^{(j)}}\bigg] *
\bigg[\frac{\partial~a_{2}^{(j)}}{\partial~z_{2}^{(j)}}~\frac{\partial~a_{3}^{(j)}}{\partial~z_{3}^{(j)}}~\frac{\partial~a_{4}^{(j)}}{\partial~z_{4}^{(j)}}~&hellip;\frac{\partial~a_{n_j+1}^{(j)}}{\partial~z_{n_j+1}^{(j)}}\bigg]
$$</p>
<p><strong><u>Step 2.</u> Adjust the weights using gradient descent</strong></p>
<p>Given $\Theta_{pq}^{(j)}$ as the weight maps from the $p^{th}$ unit of layer $j$ to the $q^{th}$ unit of layer $(j+1)$, the gradient $g$ of weight $\Theta_{pq}^{(j)}$ can be written as</p>
<p>$$
g_{\Theta_{pq}^{(j)}} = \frac{\partial~E_{total}}{\partial~\Theta_{pq}^{(j)}} = \frac{\partial~E_{total}}{\partial~z_{q}^{(j)}} \times \frac{\partial~z_{q}^{(j+1)}}{\partial~\Theta_{pq}^{(j)}}
= E_{z_q^{(j+1)}} \times a_{p}^{(j)}
$$</p>
<p>with the fact that $E_{z_q^{(j+1)}}$ for all units have been calculated in the previous step.   Next, the weights would be updated according to the following rule</p>
<p>$$
w \leftarrow w - \eta * \frac{\partial~E_{total}}{\partial~w}
$$</p>
<p>($w$: weight; $\eta$: learning rate)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Finally, we adjust the weights, using the backpropagation rules</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)):</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">arch</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">delta_vec</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">arch</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">layer</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>For a certain layer $j$, the <code>layer.T.dot(delta)</code> representation in the last line of the code above can be illustrated as</p>
<p>$$
\begin{bmatrix}
a_1^{(j)} \<br>
a_2^{(j)} \<br>
a_3^{(j)} \<br>
\vdots \<br>
a_{(n_j+1)}^{(j)}
\end{bmatrix}
\cdot
\begin{bmatrix}
E_{z_2^{(j)}} &amp; E_{z_3^{(j)}} &amp; E_{z_4^{(j)}} &amp; \dots  &amp; E_{z_{n_{j+1}}^{(j)}}
\end{bmatrix}
$$</p>
<h2 id="ii-results">II. Results</h2>
<p>Now we can check if this simple Neural Network can actually learn <code>XOR</code> rule, which is</p>
<table>
<thead>
<tr>
<th></th>
<th>y=0</th>
<th>y=1</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>x=0</strong></td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td><strong>x=1</strong></td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>In other words, we are going to use the follwoing input to train our neural network:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Set the input data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Set the labels, the correct results for the xor operation</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> 
                 <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="a-experiment-on-neural-network-with-1-hidden-layer">A. Experiment on Neural Network with 1 Hidden Layer</h3>
<p>First, we examine if a neural network with only 1 hidden layer can be trained as a XOR function.   To create a neural network that fit with the input/output data dimension, we initialize our <code>NeuralNetwork</code> with</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Initialize the NeuralNetwork with</span>
<span class="c1"># 2 input neurons</span>
<span class="c1"># 2 hidden neurons</span>
<span class="c1"># 1 output neuron</span>
<span class="n">nn</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</code></pre></td></tr></table>
</div>
</div><p>We then train this initialized <code>NeuralNetwork</code> object with <code>100000</code> epochs and exmine if the trained model predicts well with the training data.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Call the fit function and train the network for a chosen number of epochs</span>
<span class="n">nn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>

<span class="c1"># Show the prediction results</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;input:</span><span class="si">{}</span><span class="s2">   prediction:</span><span class="si">{}</span><span class="s2">   truth:</span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><p>The experiment result would be</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-sh" data-lang="sh">epochs: <span class="m">10000</span>
epochs: <span class="m">20000</span>
epochs: <span class="m">30000</span>
epochs: <span class="m">40000</span>
epochs: <span class="m">50000</span>
epochs: <span class="m">60000</span>
epochs: <span class="m">70000</span>
epochs: <span class="m">80000</span>
epochs: <span class="m">90000</span>
epochs: <span class="m">100000</span>
input:<span class="o">[</span><span class="m">0</span> 0<span class="o">]</span>   prediction:<span class="o">[</span> 0.04294694<span class="o">]</span>   truth:0
input:<span class="o">[</span><span class="m">0</span> 1<span class="o">]</span>   prediction:<span class="o">[</span> 0.96074564<span class="o">]</span>   truth:1
input:<span class="o">[</span><span class="m">1</span> 0<span class="o">]</span>   prediction:<span class="o">[</span> 0.96069096<span class="o">]</span>   truth:1
input:<span class="o">[</span><span class="m">1</span> 1<span class="o">]</span>   prediction:<span class="o">[</span> 0.04572803<span class="o">]</span>   truth:0
</code></pre></td></tr></table>
</div>
</div><h3 id="b-experiment-on-neural-network-with-2-hidden-layers">B. Experiment on Neural Network with 2 Hidden Layers</h3>
<p>Next, we examine how a neural network with 2 hidden layers performs when being trained as a XOR function.   This time we initialize our <code>NeuralNetwork</code> with</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Initialize the NeuralNetwork with</span>
<span class="c1"># 2 input neurons</span>
<span class="c1"># 2 neurons in the first hidden layer</span>
<span class="c1"># 2 neurons in the second hidden layer</span>
<span class="c1"># 1 output neuron</span>
<span class="n">nn</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</code></pre></td></tr></table>
</div>
</div><p>We then train this initialized <code>NeuralNetwork</code> object with <code>100000</code> epochs and exmine if the trained model predicts well with the training data.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Call the fit function and train the network for a chosen number of epochs</span>
<span class="n">nn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>

<span class="c1"># Show the prediction results</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;input:</span><span class="si">{}</span><span class="s2">   prediction:</span><span class="si">{}</span><span class="s2">   truth:</span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><p>The experiment result would be</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-sh" data-lang="sh">epochs: <span class="m">10000</span>
epochs: <span class="m">20000</span>
epochs: <span class="m">30000</span>
epochs: <span class="m">40000</span>
epochs: <span class="m">50000</span>
epochs: <span class="m">60000</span>
epochs: <span class="m">70000</span>
epochs: <span class="m">80000</span>
epochs: <span class="m">90000</span>
epochs: <span class="m">100000</span>
input:<span class="o">[</span><span class="m">0</span> 0<span class="o">]</span>   prediction:<span class="o">[</span> 0.02712772<span class="o">]</span>   truth:0
input:<span class="o">[</span><span class="m">0</span> 1<span class="o">]</span>   prediction:<span class="o">[</span> 0.96895982<span class="o">]</span>   truth:1
input:<span class="o">[</span><span class="m">1</span> 0<span class="o">]</span>   prediction:<span class="o">[</span> 0.96879823<span class="o">]</span>   truth:1
input:<span class="o">[</span><span class="m">1</span> 1<span class="o">]</span>   prediction:<span class="o">[</span> 0.0318513<span class="o">]</span>   truth:0
</code></pre></td></tr></table>
</div>
</div><h2 id="iii-discussions">III. Discussions</h2>
<p>To dicuss further, I use decision region maps to see how different neural networks separate different regions depending on the architecture chosen.</p>
<p>Different neural network architectures (for example, implementing a network with a different number of neurons in the hidden layer, or with more than just one hidden layer) may produce a different separating region.</p>
<p>For example, <code>[2,2,1]</code> will represent a 3-layer neural network, with two neurons in the first and the only hidden layer, and choosing it will give the following figure:</p>
<p><img src="https://i.imgur.com/KyFxXH9.png" alt="Predictions of the 1-hidden-layer model"></p>
<p>We can see that after 40000 epochs, our <code>NeuralNetwork</code> can already predict well as a <code>XOR</code> function.</p>
<p>On the other hand,  <code>[2,2,2,1]</code> will represent a 3-layer neural network, with two neurons in the first only hidden layer and two neurons in the second hidden layer. Choosing it will give the following figure:</p>
<p><img src="https://i.imgur.com/8cQUbEr.png" alt="Predictions of the 2-hidden-layer model"></p>
<p>We can see that as a neural network that is more complex, it is not yet well trained when the number of epochs is 40000.   That is, it takes more time to train a more complex neural network.</p>
<h2 id="iv-appendix">IV. Appendix</h2>
<ol>
<li>The full implementation code</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">:</span>
    <span class="c1">#########</span>
    <span class="c1"># parameters</span>
    <span class="c1"># ----------</span>
    <span class="c1"># self:      the class object itself</span>
    <span class="c1"># net_arch:  consists of a list of integers, indicating</span>
    <span class="c1">#            the number of neurons in each layer, i.e. the network architecture</span>
    <span class="c1">#########</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net_arch</span><span class="p">):</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Initialized the weights, making sure we also </span>
        <span class="c1"># initialize the weights for the biases that we will add later</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activity</span> <span class="o">=</span> <span class="n">sigmoid</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activity_derivative</span> <span class="o">=</span> <span class="n">sigmoid_derivative</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">net_arch</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">arch</span> <span class="o">=</span> <span class="n">net_arch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Random initialization with range of weight values (-1,1)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">w</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">net_arch</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">net_arch</span><span class="p">[</span><span class="n">layer</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_forward_prop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">activity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activity</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>

            <span class="c1"># add the bias for the next layer</span>
            <span class="n">activity</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">activity</span><span class="p">)))</span>
            <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activity</span><span class="p">)</span>

        <span class="c1"># last layer</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">activity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activity</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activity</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">y</span>
    
    <span class="k">def</span> <span class="nf">_back_prop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">target</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">delta_vec</span> <span class="o">=</span> <span class="p">[</span><span class="n">error</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">activity_derivative</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>

        <span class="c1"># we need to begin from the back, from the next to last layer</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">delta_vec</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">error</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">activity_derivative</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">:])</span>
            <span class="n">delta_vec</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>

        <span class="c1"># Now we need to set the values from back to front</span>
        <span class="n">delta_vec</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
        
        <span class="c1"># Finally, we adjust the weights, using the backpropagation rules</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)):</span>
            <span class="n">layer</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">delta_vec</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">layer</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>
    
    <span class="c1">#########</span>
    <span class="c1"># parameters</span>
    <span class="c1"># ----------</span>
    <span class="c1"># self:    the class object itself</span>
    <span class="c1"># data:    the set of all possible pairs of booleans True or False indicated by the integers 1 or 0</span>
    <span class="c1"># labels:  the result of the logical operation &#39;xor&#39; on each of those input pairs</span>
    <span class="c1">#########</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        
        <span class="c1"># Add bias units to the input layer - </span>
        <span class="c1"># add a &#34;1&#34; to the input data (the always-on bias neuron)</span>
        <span class="n">ones</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">ones</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">data</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epochs: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
        
            <span class="n">sample</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

            <span class="c1"># We will now go ahead and set up our feed-forward propagation:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">Z</span><span class="p">[</span><span class="n">sample</span><span class="p">]]</span>
            <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_prop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

            <span class="c1"># Now we do our back-propagation of the error to adjust the weights:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">sample</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_back_prop</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
    
    <span class="c1">#########</span>
    <span class="c1"># the predict function is used to check the prediction result of</span>
    <span class="c1"># this neural network.</span>
    <span class="c1"># </span>
    <span class="c1"># parameters</span>
    <span class="c1"># ----------</span>
    <span class="c1"># self:   the class object itself</span>
    <span class="c1"># x:      single input data</span>
    <span class="c1">#########</span>
    <span class="k">def</span> <span class="nf">predict_single_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">val</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)):</span>
            <span class="n">val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activity</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
            <span class="n">val</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">val</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">val</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1">#########</span>
    <span class="c1"># the predict function is used to check the prediction result of</span>
    <span class="c1"># this neural network.</span>
    <span class="c1"># </span>
    <span class="c1"># parameters</span>
    <span class="c1"># ----------</span>
    <span class="c1"># self:   the class object itself</span>
    <span class="c1"># X:      the input data array</span>
    <span class="c1">#########</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_single_data</span><span class="p">(</span><span class="n">x</span><span class="p">)]])</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">Y</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">Y</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
    <span class="c1"># Initialize the NeuralNetwork with</span>
    <span class="c1"># 2 input neurons</span>
    <span class="c1"># 2 hidden neurons</span>
    <span class="c1"># 1 output neuron</span>
    <span class="n">nn</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Set the input data</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

    <span class="c1"># Set the labels, the correct results for the xor operation</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> 
                    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Call the fit function and train the network for a chosen number of epochs</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>

    <span class="c1"># Show the prediction results</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;input:</span><span class="si">{}</span><span class="s2">   prediction:</span><span class="si">{}</span><span class="s2">   truth:</span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>The self-defined plot functions are written here.</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">def</span> <span class="nf">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> <span class="n">test_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mf">0.02</span><span class="p">):</span>

    <span class="c1"># setup marker generator and color map</span>
    <span class="n">markers</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">)</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;lightgreen&#39;</span><span class="p">,</span> <span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="s1">&#39;cyan&#39;</span><span class="p">)</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">colors</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))])</span>

    <span class="c1"># plot the decision surface</span>
    <span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">),</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xx1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">xx2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx1</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx1</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">xx2</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx2</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>

    <span class="c1"># plot class samples</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">cl</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">cl</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">cl</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idx</span><span class="p">),</span>
                    <span class="n">marker</span><span class="o">=</span><span class="n">markers</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">cl</span><span class="p">)</span>

    <span class="c1"># highlight test samples</span>
    <span class="k">if</span> <span class="n">test_idx</span><span class="p">:</span>
        <span class="c1"># plot all samples</span>
        <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">test_idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
                    <span class="n">X_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="n">c</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                    <span class="n">linewidths</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span>
                    <span class="n">s</span><span class="o">=</span><span class="mi">55</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test set&#39;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="v-references">V. References</h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Sigmoid_function">Wikipedia - Sigmoid function</a></li>
<li><a href="https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_backpropagation_work.html">How backpropagation works</a></li>
<li><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">A Step by Step Backpropagation Example</a></li>
</ul>
    
	</div>
  <footer class="article-footer clearfix">
  

<div class="article-tags">
  <span></span>
  
  <a href="https://binnz.github.io/tags/python">Python</a>
  
  <a href="https://binnz.github.io/tags/neural-network">Neural-Network</a>
  
  <a href="https://binnz.github.io/tags/deep-learning">Deep-Learning</a>
  
  <a href="https://binnz.github.io/tags/data-mining">Data-Mining</a>
  
</div>





<div class="article-categories">
  <span></span>
  
  <a class="article-category-link" href="https://binnz.github.io/categories/programming">Programming</a>
  
</div>



  <div class="article-share" id="share">
    <div data-url="https://binnz.github.io/post/2018-08-19-nn-xor/" data-title="Neural Network Architecture and Back-Propagation" data-tsina="" class="share clearfix">
    </div>
  </div>
</footer>

	</article>
  


<section class="comment">
<div id="disqus_thread"></div>
</section>
<script>
  <!-- detect whether Disuqs can load -->
  var xhr = new XMLHttpRequest();
  xhr.open('GET', '//disqus.com/next/config.json?' + new Date().getTime(), true);
  xhr.timeout = 3000; 

  xhr.onload = function() { 


var disqus_config = function () {
this.page.url = "https://binnz.github.io/post/2018-08-19-nn-xor/";
this.page.identifier = "https://binnz.github.io/post/2018-08-19-nn-xor/";
};
(function() { 
var d = document, s = d.createElement('script');

s.src = '//disc.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
}
  xhr.ontimeout = function() {
  <!-- cannot load Disqus, skip it. -->
  return;
}
xhr.send(null);
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


</div>

    <div class="openaside"><a class="navbutton" href="#" title="Show SideBar"></a></div>
<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide SideBar"></a></div>
<aside class="clearfix">
  

<div class="categorieslist">
  <p class="asidetitle">Categories</p>
  <ul>
    
    <li><a href="https://binnz.github.io/categories/install" title="install">install<sup>2</sup></a></li>
    
    <li><a href="https://binnz.github.io/categories/life" title="life">life<sup>1</sup></a></li>
    
    <li><a href="https://binnz.github.io/categories/notes" title="notes">notes<sup>50</sup></a></li>
    
    <li><a href="https://binnz.github.io/categories/programming" title="programming">programming<sup>25</sup></a></li>
    
    <li><a href="https://binnz.github.io/categories/projects" title="projects">projects<sup>2</sup></a></li>
    
    <li><a href="https://binnz.github.io/categories/publications" title="publications">publications<sup>1</sup></a></li>
    
  </ul>
</div>



  

<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
      
			<li><a href="https://binnz.github.io/tags/ANDROID" title="android">android<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/APACHE-PIG" title="apache-pig">apache-pig<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/ASSOCIATION-ANALYSIS" title="association-analysis">association-analysis<sup>9</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/BEER" title="beer">beer<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/CLASSIFICATION" title="classification">classification<sup>9</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/CLUSTERING" title="clustering">clustering<sup>19</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/CONFERENCE" title="conference">conference<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/DATA-MINING" title="data-mining">data-mining<sup>64</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/DATA-SCIENCE" title="data-science">data-science<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/DATABASE" title="database">database<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/DEEP-LEARNING" title="deep-learning">deep-learning<sup>2</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/ELASTICSEARCH" title="elasticsearch">elasticsearch<sup>2</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/GRAPH" title="graph">graph<sup>12</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/HADOOP" title="hadoop">hadoop<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/INFORMATION-RETRIEVAL" title="information-retrieval">information-retrieval<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/JAVA" title="java">java<sup>5</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/MACHINE-LEARNING" title="machine-learning">machine-learning<sup>5</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/NEURAL-NETWORK" title="neural-network">neural-network<sup>2</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/NLP" title="nlp">nlp<sup>4</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/NLTK" title="nltk">nltk<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/PYTHON" title="python">python<sup>15</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/R" title="r">r<sup>4</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/RELATION-ALGEBRA" title="relation-algebra">relation-algebra<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/SCIKIT-LEARN" title="scikit-learn">scikit-learn<sup>5</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/SPARK" title="spark">spark<sup>9</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/SQL" title="sql">sql<sup>3</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/STATISTICS" title="statistics">statistics<sup>5</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/STREAMING" title="streaming">streaming<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/TIME-SERIES" title="time-series">time-series<sup>2</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/TRAVEL" title="travel">travel<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/UNIT-TEST" title="unit-test">unit-test<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/VBA" title="vba">vba<sup>1</sup></a></li>
      
			<li><a href="https://binnz.github.io/tags/WINDOWS" title="windows">windows<sup>1</sup></a></li>
      
		</ul>
</div>



  
  <div class="archiveslist">
    <p class="asidetitle">Archives</p>
    <ul class="archive-list">
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2021-06">2021年06月</a><span class="archive-list-count">1</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2020-05">2020年05月</a><span class="archive-list-count">12</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2019-09">2019年09月</a><span class="archive-list-count">1</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2018-09">2018年09月</a><span class="archive-list-count">4</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2018-08">2018年08月</a><span class="archive-list-count">1</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2018-06">2018年06月</a><span class="archive-list-count">2</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2018-05">2018年05月</a><span class="archive-list-count">4</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2018-04">2018年04月</a><span class="archive-list-count">5</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2018-03">2018年03月</a><span class="archive-list-count">5</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2018-01">2018年01月</a><span class="archive-list-count">2</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2017-12">2017年12月</a><span class="archive-list-count">1</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2017-11">2017年11月</a><span class="archive-list-count">1</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2017-10">2017年10月</a><span class="archive-list-count">1</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2017-09">2017年09月</a><span class="archive-list-count">10</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2017-08">2017年08月</a><span class="archive-list-count">2</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2017-07">2017年07月</a><span class="archive-list-count">4</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2017-05">2017年05月</a><span class="archive-list-count">3</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2017-04">2017年04月</a><span class="archive-list-count">2</span>
      </li>
      
      
      <li class="archive-list-item">
        <a class="archive-list-link" href="https://binnz.github.io/post/#2017-03">2017年03月</a><span class="archive-list-count">20</span>
      </li>
      
    </ul>

  </div>


  

<div class="tagcloudlist">
  <p class="asidetitle">Tags Cloud</p>
  <div class="tagcloudlist clearfix">
    
    <a href="https://binnz.github.io/tags/android" style="font-size: 12px;">android</a>
    
    <a href="https://binnz.github.io/tags/apache-pig" style="font-size: 12px;">apache-pig</a>
    
    <a href="https://binnz.github.io/tags/association-analysis" style="font-size: 12px;">association-analysis</a>
    
    <a href="https://binnz.github.io/tags/beer" style="font-size: 12px;">beer</a>
    
    <a href="https://binnz.github.io/tags/classification" style="font-size: 12px;">classification</a>
    
    <a href="https://binnz.github.io/tags/clustering" style="font-size: 12px;">clustering</a>
    
    <a href="https://binnz.github.io/tags/conference" style="font-size: 12px;">conference</a>
    
    <a href="https://binnz.github.io/tags/data-mining" style="font-size: 12px;">data-mining</a>
    
    <a href="https://binnz.github.io/tags/data-science" style="font-size: 12px;">data-science</a>
    
    <a href="https://binnz.github.io/tags/database" style="font-size: 12px;">database</a>
    
    <a href="https://binnz.github.io/tags/deep-learning" style="font-size: 12px;">deep-learning</a>
    
    <a href="https://binnz.github.io/tags/elasticsearch" style="font-size: 12px;">elasticsearch</a>
    
    <a href="https://binnz.github.io/tags/graph" style="font-size: 12px;">graph</a>
    
    <a href="https://binnz.github.io/tags/hadoop" style="font-size: 12px;">hadoop</a>
    
    <a href="https://binnz.github.io/tags/information-retrieval" style="font-size: 12px;">information-retrieval</a>
    
    <a href="https://binnz.github.io/tags/java" style="font-size: 12px;">java</a>
    
    <a href="https://binnz.github.io/tags/machine-learning" style="font-size: 12px;">machine-learning</a>
    
    <a href="https://binnz.github.io/tags/neural-network" style="font-size: 12px;">neural-network</a>
    
    <a href="https://binnz.github.io/tags/nlp" style="font-size: 12px;">nlp</a>
    
    <a href="https://binnz.github.io/tags/nltk" style="font-size: 12px;">nltk</a>
    
    <a href="https://binnz.github.io/tags/python" style="font-size: 12px;">python</a>
    
    <a href="https://binnz.github.io/tags/r" style="font-size: 12px;">r</a>
    
    <a href="https://binnz.github.io/tags/relation-algebra" style="font-size: 12px;">relation-algebra</a>
    
    <a href="https://binnz.github.io/tags/scikit-learn" style="font-size: 12px;">scikit-learn</a>
    
    <a href="https://binnz.github.io/tags/spark" style="font-size: 12px;">spark</a>
    
    <a href="https://binnz.github.io/tags/sql" style="font-size: 12px;">sql</a>
    
    <a href="https://binnz.github.io/tags/statistics" style="font-size: 12px;">statistics</a>
    
    <a href="https://binnz.github.io/tags/streaming" style="font-size: 12px;">streaming</a>
    
    <a href="https://binnz.github.io/tags/time-series" style="font-size: 12px;">time-series</a>
    
    <a href="https://binnz.github.io/tags/travel" style="font-size: 12px;">travel</a>
    
    <a href="https://binnz.github.io/tags/unit-test" style="font-size: 12px;">unit-test</a>
    
    <a href="https://binnz.github.io/tags/vba" style="font-size: 12px;">vba</a>
    
    <a href="https://binnz.github.io/tags/windows" style="font-size: 12px;">windows</a>
    
  </div>
</div>



  

</aside>
</div>

  </div>
  <footer><div id="footer" >
  <div class="line">
    <span></span>
    
    <div style='background:no-repeat url("https://binnz.github.io/img/US-MT-EPS-02-6001.png") left top;-webkit-background-size:6.875em 6.875em;-moz-background-size:6.875em 6.875em;background-size:6.875em 6.875em;' class="author" ></div>
  </div>
  <section class="info">
    <p>hehe</p>
  </section>
  <div class="social-font clearfix">
    <a href='http://weibo.com/coderzh' target="_blank" title="weibo"></a>
    <a href='https://twitter.com/coderzh' target="_blank" title="twitter"></a>
    <a href='https://github.com/binnz' target="_blank" class="icon-github" title="github"></a>
    <a href='https://www.facebook.com/coderzh' target="_blank" title="facebook"></a>
    <a href='https://www.linkedin.com/coderzh' target="_blank" title="linkedin"></a>
  </div>
  <p class="copyright">Powered by <a href="http://gohugo.io" target="_blank" title="hugo">hugo</a> and Theme by <a href="https://github.com/coderzh/hugo-pacman-theme" target="_blank" title="hugo-pacman-theme">hugo-pacman-theme</a> © 2021
    
    <a href="https://binnz.github.io/" title="California Dreaming">California Dreaming</a>
    
  </p>
</div>
</footer>
  <script src="https://binnz.github.io/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
done = false;
$(document).ready(function(){
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize();
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  $('form.search').on('submit', function (event) {
    if (false === done) {
      event.preventDefault();
      var orgVal = $(this).find('#search').val();
      $(this).find('#search').val('site:https:\/\/binnz.github.io\/ ' + orgVal);
      done = true;
      $(this).submit();
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{
    c.click(function(){
      ta.css('display', 'block').addClass('fadeIn');
    });
    o.click(function(){
      ta.css('display', 'none');
    });
    $(window).scroll(function(){
      ta.css("top",Math.max(140,320-$(this).scrollTop()));
    });
  };
});
</script>

<script type="text/javascript">
$(document).ready(function(){
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://b.bshare.cn/barCode?site=weixin&url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});
</script>





</body>
</html>
