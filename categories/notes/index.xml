<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on California Dreaming</title>
    <link>https://binnz.github.io/categories/notes/</link>
    <description>Recent content in Notes on California Dreaming</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 01 Jun 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://binnz.github.io/categories/notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introduction to Database</title>
      <link>https://binnz.github.io/post/2021-06-01-database/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2021-06-01-database/</guid>
      <description>&lt;p&gt;What is Database Management System(DBMS)?
In short, a DBMS should support the following functionalities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;store data (large amount of data!)&lt;/li&gt;
&lt;li&gt;query data&lt;/li&gt;
&lt;li&gt;modify data (add, update, delete, &amp;hellip;)&lt;/li&gt;
&lt;li&gt;enable durability (recover from failure)&lt;/li&gt;
&lt;li&gt;control access of many users
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;isolation&lt;/em&gt; (each transaction appears to execute in isolation from other transactions)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;atomicity&lt;/em&gt; (prevents updates to the database occurring only partially)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Allow users to create new databases and specify their schemas using a data definition language&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Community Detection in Graphs</title>
      <link>https://binnz.github.io/post/2020-05-25-graph-communities/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2020-05-25-graph-communities/</guid>
      <description>&lt;p&gt;In a graph structure, what can be called a community?
The notion of community structure captures the tendency of nodes to be organized into groups and members within a community are more similar among each other.
Typically, a community in graphs/networks is a set of nodes with more/better/stronger connections between its members, than to the rest of the network.
However, there is no widely accepted single definition.
It depends heavily on the application domain and the properies of the graph.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Link Prediction</title>
      <link>https://binnz.github.io/post/2020-05-25-link-prediction/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2020-05-25-link-prediction/</guid>
      <description>&lt;p&gt;What is Link Prediction?
Given a snapshot of a network at time $t$, predict edges added in the interval $(t,t&#39;)$&lt;/p&gt;
&lt;!-- Type of Predictions:
- Link existance (binary classification problem)
- Link weight (regression problem). E.g., predicting movie rating for users
- Link type (multi-class classification problem) --&gt;</description>
    </item>
    
    <item>
      <title>Label Propagation</title>
      <link>https://binnz.github.io/post/2020-05-24-label-propagation/</link>
      <pubDate>Sun, 24 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2020-05-24-label-propagation/</guid>
      <description>&lt;p&gt;Labelling/Annotating data is very expensive.
Therefore, usualy we have only small amounts of labelled data and large amounts of unlabelled data.
To predict the labels fo the unlabelled data, we can use the graph-based semi-supervised machine learning technique called &lt;strong&gt;Label Propagation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So what is label propagation?&lt;/p&gt;
&lt;p&gt;Label Propagation Algorithm (LPA) is an iterative algorithm where we assign labels to unlabelled points by propagating labels through the dataset.
For example, if we are given a graph with partially labelled nodes, for any unlabelled node, we can either adopt the dominant label in its neighborhood or wait until a label &amp;ldquo;propagates&amp;rdquo; to it if its neighborhood does not have any label.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Web Graph and Page Ranking</title>
      <link>https://binnz.github.io/post/2020-05-22-pagerank/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2020-05-22-pagerank/</guid>
      <description>&lt;p&gt;In this post, we first discuss the structure of the Web as a graph consisting a large number of pages and connected by hyperlinks.
Then to rank the pages in the World Wide Web, we introduce PageRank and talk about some of its weakness as a general approach.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Random Walks on Graphs</title>
      <link>https://binnz.github.io/post/2020-05-20-random-walks/</link>
      <pubDate>Wed, 20 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2020-05-20-random-walks/</guid>
      <description>&lt;p&gt;A random walk is known as a stochastic or random process which describes a path that consists of a succession of random steps on some mathematical space:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;given a graph and a starting point, select a neighbour at random&lt;/li&gt;
&lt;li&gt;move to the selected neighbour and repeat the same process till a termination condition is verified&lt;/li&gt;
&lt;li&gt;the random sequence of points selected in this way is a &lt;em&gt;random walk&lt;/em&gt; of the graph&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>A Real-World Example of Network Graph</title>
      <link>https://binnz.github.io/post/2020-05-15-network-example/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2020-05-15-network-example/</guid>
      <description>&lt;p&gt;In June 2006, MSN Messenger had 30 billion conversations among 240 million people.
From the data, the MSN network is constructed as a communication graph with $n = 180 \text{ million}$ nodes and $m = 1.3 \text{ billion}$ undirected edges.
To investigate the properties of large-scale networks, we take the real-world data, MSN network, as an example and discover its macroscopic properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Diameter&lt;/strong&gt; - &lt;em&gt;What is the degrees of separation in this network?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clustering coefficient&lt;/strong&gt; - &lt;em&gt;What fraction of my friends are also friends themselves?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Degree distribution&lt;/strong&gt; - &lt;em&gt;Are there many &amp;ldquo;hubs&amp;rdquo; in the network?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Connectivity&lt;/strong&gt; - &lt;em&gt;How many &amp;ldquo;islands&amp;rdquo; and how big are they?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- - **Navigability** - _Can you do efficient routing?_ --&gt;</description>
    </item>
    
    <item>
      <title>Erdos-Renyi Random Graph</title>
      <link>https://binnz.github.io/post/2020-05-15-gnp/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2020-05-15-gnp/</guid>
      <description>&lt;p&gt;$G(n, p)$, the Erdos-Renyi Random Graph, defines a family of graphs, each of which starts with $n$ isolated nodes, and we place an edge between each distinct node pair with probability $p$.
In $G(n, p)$ Model, the probability of obtaining any one particular random graph with $m$ edges is $p^{m}(1-p)^{N-m}$ with the notation $N=\binom{n}{2}$.
As a result, $G(n, p)$ defines a bigger familiy than $G(n, m)$ since $n$ and $p$ do not uniquely determine the graph so number of possible graphs are larger.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kleinberg&#39;s Model of Small-Worlds</title>
      <link>https://binnz.github.io/post/2020-05-15-kleinberg/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2020-05-15-kleinberg/</guid>
      <description>&lt;p&gt;Kleinberg&amp;rsquo;s model presents the infinite family of &lt;strong&gt;navigable Small-World networks&lt;/strong&gt; that generalizes Watts-Strogatz model.
Moreover, with Kleinberg&amp;rsquo;s model it is shown that &lt;strong&gt;short paths not only exist but can be found with limited knowledge of the global network&lt;/strong&gt;. Decentralized search algorithms can find short paths with high probability.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Random Graph Models</title>
      <link>https://binnz.github.io/post/2020-05-15-graph-models/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2020-05-15-graph-models/</guid>
      <description>&lt;p&gt;In mathematics, random graph is the general term to refer to probability distributions over graphs.
Random graphs may be described simply by a probability distribution, or by a random process which generates them (Bollob√°s 2001).
From a mathematical perspective, random graphs are found to model and mirror the diverse types of complex networks encountered in different areas.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Watts-Strogatz Model of Small-Worlds</title>
      <link>https://binnz.github.io/post/2020-05-15-watts-strogatz/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2020-05-15-watts-strogatz/</guid>
      <description>&lt;p&gt;The Watts-Strogatz model is a random graph generation model that produces graphs with &lt;strong&gt;small-world properties&lt;/strong&gt;, including &lt;strong&gt;short average path lengths&lt;/strong&gt; and &lt;strong&gt;high clustering&lt;/strong&gt;.
To check the simulation of a small world model, &lt;a href=&#34;http://netlogoweb.org/launch#http://netlogoweb.org/assets/modelslib/Sample%20Models/Networks/Small%20Worlds.nlogo&#34;&gt;this website&lt;/a&gt; is very helpful.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Centrality Measures of Graph Nodes</title>
      <link>https://binnz.github.io/post/2020-05-14-centrality/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2020-05-14-centrality/</guid>
      <description>&lt;p&gt;Suppose we are given a graph data $G = (N, E)$, which contains $|N| = n$ nodes and $|E| = m$ edges.
One big question we often ask is: &amp;ldquo;Which vertices are important?&amp;rdquo;
Intuitively, we would consider a &amp;ldquo;star&amp;rdquo; (the central node that connects a lot of other nodes) as an obviously important case and also consider nodes in a &amp;ldquo;circle&amp;rdquo; are equivalently important.
In general, we can use cantrality measures to rank the nodes in a graph.
There are many centrality measures and page rank is currently the most prominent approach that deals with directed graphs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Graph Fundamentals</title>
      <link>https://binnz.github.io/post/2020-05-12-graph/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2020-05-12-graph/</guid>
      <description>&lt;p&gt;The data reflecting our real world can be represented in networks sometimes.   With network data, we can ask questions such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the patterns and statistical properties of network data?   Why networks are the way they are? Can we find the underlying rules that build these networks?&lt;/li&gt;
&lt;li&gt;Can we model the networks?   Can we predict behavior? Why/How things go viral?&lt;/li&gt;
&lt;li&gt;How does the network structure evolve over time?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To answer the questions, we need to understand network properties (e.g., diameter, scale-free / power law network, small-world behavior), network models that fit our observations (e.g., Erdos Renyi random graphs, Kleinberg‚Äôs model models, &amp;hellip; etc), and algorithms that could unflod on our networks (e.g., page rank, decentralized search, label propagation, link prediction, community detection, &amp;hellip; etc).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Cluster Validaty and Cluster Number Selection</title>
      <link>https://binnz.github.io/post/2018-05-28-cluster-number/</link>
      <pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-05-28-cluster-number/</guid>
      <description>&lt;p&gt;Numbers of cluster validity measures have been proposed to help us not only with the validation of our clustering result but also with cluster number selection.&lt;/p&gt;
&lt;p&gt;For fuzzy clustering, we can optimize our clustering results with some validity measure such as Partition Coefficient, Partition Entropy, XB-index, and Overlaps Separation Measure.&lt;/p&gt;
&lt;p&gt;For hard clustering, we can use measures such as DB index and Dunn index.&lt;/p&gt;
&lt;p&gt;And for hierarchical clustering, we can select the best $k$ by stopping at the &amp;ldquo;Big Jump&amp;rdquo; of distances while performing agglomerative clustering.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Aggregation of Clustering Methods</title>
      <link>https://binnz.github.io/post/2018-05-21-clustering-integration/</link>
      <pubDate>Mon, 21 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-05-21-clustering-integration/</guid>
      <description>&lt;p&gt;Since a large number of clustering algorithms exist, aggregating different clustered partitions into a single consolidated one to obtain better results has become an important problem.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Categorical Data Clustering</title>
      <link>https://binnz.github.io/post/2018-05-07-k-modes/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-05-07-k-modes/</guid>
      <description>&lt;p&gt;Categorical Data Clustering, including k-modes and ROCK, will be introduced in this document.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Quick-Finding of the Nearest Center</title>
      <link>https://binnz.github.io/post/2018-04-26-1nn/</link>
      <pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-04-26-1nn/</guid>
      <description>&lt;p&gt;Quick-Finding of the Nearest Center is used for&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fast k-means&lt;/li&gt;
&lt;li&gt;Fast VQ (accelerating the compression speed of VQ)&lt;/li&gt;
&lt;li&gt;Fast classification&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Probabilistic D-Clustering</title>
      <link>https://binnz.github.io/post/2018-04-16-pdc/</link>
      <pubDate>Mon, 16 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-04-16-pdc/</guid>
      <description>&lt;p&gt;Probabilistic D-Clustering is a new iterative method for probabilistic clustering of data.   Given clusters, their centers and the distances of data points from these centers, the probability of cluster membership at any point is assumed inversely proportional to the distance from (the center of) the cluster. This assumption is the working principle of Probabilistic D-Clustering.&lt;/p&gt;
&lt;p&gt;At each iteration, the distances (Euclidean, Mahalanobis, etc.) from the cluster centers are computed for all data points, and the centers are updated as convex combinations of these points, with weights determined by the above principle.&lt;/p&gt;
&lt;p&gt;Progress is monitored by the joint distance function, a measure of distance from all cluster centers, that evolves during the iterations, and captures the data in its low contours.&lt;/p&gt;
&lt;p&gt;This method is simple, fast (requiring a small number of cheap iterations) and insensitive to outliers.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Vector Quantization and Its Variants</title>
      <link>https://binnz.github.io/post/2018-04-09-vector-quantization/</link>
      <pubDate>Mon, 09 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-04-09-vector-quantization/</guid>
      <description>&lt;p&gt;The goal of Vector Quantization(VQ) is to perform compression and decompression using a given codebook.   So, annother question is, &lt;em&gt;&amp;ldquo;How to generate a codebook?&amp;quot;&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Analytical Data Clustering</title>
      <link>https://binnz.github.io/post/2018-04-02-analytical-clustering/</link>
      <pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-04-02-analytical-clustering/</guid>
      <description>&lt;p&gt;Analytical clustering is a quick and automatic way by preserving certain features of the input data. The method is analytical, deterministic, unsupervised, automatic, and noniterative.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Use Annealing Method to Avoid Local Minimum</title>
      <link>https://binnz.github.io/post/2018-04-02-annealing-clustering/</link>
      <pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-04-02-annealing-clustering/</guid>
      <description>&lt;p&gt;Local minima are still an important unsolved problem for artificial potential field approaches.   In order to overcome this problem, annealing methods are proposed to prevent being trapped in a local minimum point, and allows the computation to continue its trajectory towards the final destination.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Monothetic Clustering Method</title>
      <link>https://binnz.github.io/post/2018-03-29-monothetic-clustering/</link>
      <pubDate>Thu, 29 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-03-29-monothetic-clustering/</guid>
      <description>&lt;p&gt;Monothetic Clustering is often used in Taxonomy.   For example, when you see a strange animal, how do you know if it&amp;rsquo;s never reported before? You may need to ask $N$ True-or-False questions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q1. animal? (yes/no)&lt;/li&gt;
&lt;li&gt;Q2. with legs? (yes/no)&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So $Monthetic$ means that every time we only use &lt;strong&gt;a single attribute(variable)&lt;/strong&gt; to cluster.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Fuzzy Clustering and Fuzzy k-Means</title>
      <link>https://binnz.github.io/post/2018-03-25-fuzzy-clustering/</link>
      <pubDate>Sun, 25 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-03-25-fuzzy-clustering/</guid>
      <description>&lt;p&gt;Fuzzy clustering is the opposite of &amp;ldquo;Hard Clustering&amp;rdquo; (i.e., &amp;ldquo;Crispy Clustering&amp;rdquo;).&lt;/p&gt;
&lt;p&gt;For example, every data point $x$ would claim its percentage belongness to every cluster $C_i$ ($1 \leq i \leq K$ where $K$ is the number of clusters).   However, the report will be too long as in this type of clustering representation.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Graph-Theoretical Method for Clustering</title>
      <link>https://binnz.github.io/post/2018-03-15-graph-clustering/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-03-15-graph-clustering/</guid>
      <description>&lt;p&gt;In general, there are two steps in Graph Methods.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1.&lt;/strong&gt; Construct a graph to connect all data (e.g., Minimal Spanning Tree, Relative Neighborhood Graph, Gabrial Graph, Delaunay Triangles, &amp;hellip;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2.&lt;/strong&gt; Delete some edges which are too long (inconsistent edges)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Peak-Climbing Data Clustering</title>
      <link>https://binnz.github.io/post/2018-03-15-peak-climbing/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-03-15-peak-climbing/</guid>
      <description>&lt;p&gt;Peak-climbing is also called &amp;ldquo;mode-seeking&amp;rdquo; or &amp;ldquo;valley-seeking&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Data Mining - Anomaly Detection</title>
      <link>https://binnz.github.io/post/2017-11-07-anomaly-detection/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-11-07-anomaly-detection/</guid>
      <description>&lt;p&gt;Anomalies, or say outliers, are the set of data points that are considerably different than the remainder of the data.   Common applications of anomaly detection are credit card fraud detection, telecommunication fraud detection, network intrusion detection, fault detection, and so on.&lt;/p&gt;
&lt;p&gt;The working assumption of anomaly detection is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are considerably more ‚Äúnormal‚Äù observations than ‚Äúabnormal‚Äù observations (outliers/anomalies) in the data.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Basic Functions in Natural Language Processing</title>
      <link>https://binnz.github.io/post/2017-10-30-basic-nlp/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-10-30-basic-nlp/</guid>
      <description>&lt;p&gt;In this article, I will introduce some basic functions being uesd in Natural Language Processing using python package &lt;code&gt;nltk&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To use the package &lt;code&gt;nltk&lt;/code&gt;, you should download the library and corpus you need beforehands.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Á∑ö‰∏äÊúÉË≠∞Á≠ÜË®òËàáÂàÜ‰∫´Ôºö„ÄåDemystifying Data Science„Äç</title>
      <link>https://binnz.github.io/post/2017-09-28-demystifying-data-science/</link>
      <pubDate>Thu, 28 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-09-28-demystifying-data-science/</guid>
      <description>&lt;p&gt;„ÄåDemystifying Data Science„ÄçÊòØÁæéÂúã‰∏ÄÂ†¥ÈùûÂ∏∏Á≤æÂΩ©ÁöÑ 12 Â∞èÊôÇÂÖçË≤ªÁõ¥Êí≠Ë¨õÂ∫ßÔºåÈÇÄË´ã 28 ‰Ωç‰æÜËá™ Facebook„ÄÅAirbnb„ÄÅQuora„ÄÅEtsy„ÄÅFast.ai Á≠âÁü•Âêç‰ºÅÊ•≠ÁöÑË≥áÊ∑±Ë≥áÊñôÁßëÂ≠∏ÂÆ∂ÂàÜ‰∫´„ÄåÂ¶Ç‰ΩïËΩâËÅ∑ÈÄ≤ÂÖ•Êàê‰∏Ä‰ΩçÊï∏ÊìöÂàÜÊûêÂ∏´„Äç„ÄÇ&lt;/p&gt;
&lt;p&gt;Áî±ÊñºÁõ¥Êí≠ÊôÇÈñìÊòØÁæéÂúãÊôÇÈñìÁöÑÊó©‰∏äÂçÅÈªûÂà∞Êôö‰∏äÂçÅÈªûÔºåÂç≥ÔºåÂè∞ÁÅ£ÊôÇÈñìÁöÑÊôö‰∏äÂçÅÈªûÂà∞Ê†ºÂºèÁöÑÊó©‰∏äÂçÅÈªûÔºåÂõ†Ê≠§ÊàëÂè™Áúã‰∫ÜÊôö‰∏äÂçÅÈªûÂà∞ÂçäÂ§úÂçÅ‰∫åÈªûÂçäÂÖ±‰∫îÂ†¥ÊºîË¨õÔºå‰∏¶Á≠ÜË®ò‰∏Ä‰∫õË¨õËÄÖÂàÜ‰∫´ÁöÑÂÖßÂÆπ„ÄÇÁî±Êñº‰∏Ä‰∫õ‰æÜ‰∏çÂèäÁ¥ÄÈåÑÁöÑÁº∫ÊºèÂÖßÂÆπÊòØ‰∫ãÂæåÂÜçÊ†πÊìöË®òÊÜ∂Ë£ú‰∏äÁöÑÔºåÂõ†Ê≠§Êúâ‰∫õÂú∞ÊñπÂèØËÉΩÁî®Ë©ûÊàñË™™Ê≥ïÊúÉ‰∏çÂ§™Á≤æÊ∫ñÔºåÂ∞±Ë´ãÂ§öÂ§öÈ´îË´íÂï¶„ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper Notes: Deep Learning at Alibaba</title>
      <link>https://binnz.github.io/post/2017-09-27-deeplearning-alibaba/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-09-27-deeplearning-alibaba/</guid>
      <description>&lt;p&gt;This notes is taken from the paper&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ijcai.org/proceedings/2017/0002.pdf&#34;&gt;Rong Jin. &lt;em&gt;Deep Learning at Alibaba&lt;/em&gt;. ICJAI 2017&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this keynote, the presenter discussed the limitations of the existing deep learning techniques and shared some solutions that Alibaba chosed to address these problems.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Data Mining - Advanced Concepts and Algorithms of Cluster Analysis</title>
      <link>https://binnz.github.io/post/2017-09-24-advanced-clustering/</link>
      <pubDate>Sun, 24 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-09-24-advanced-clustering/</guid>
      <description>&lt;p&gt;Agglomerative clustering algorithms vary in terms of how the proximity of two clusters are computed. However, with MIN (single link), it is susceptible to noise/outliers; with MAX/GROUP AVERAGE, it may not work well with non-globular clusters.&lt;/p&gt;
&lt;p&gt;So how can we deal with these two types of problem?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Agglomerative Method for Hierarchical Clustering</title>
      <link>https://binnz.github.io/post/2017-09-01-agg-clustering/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-09-01-agg-clustering/</guid>
      <description>&lt;p&gt;Agglomerative clustering &lt;strong&gt;start with the points as individual clusters&lt;/strong&gt;.   At each step, it &lt;strong&gt;merges the closest pair of clusters until only one cluster (or k clusters) left&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Cluster Center Initialization Algorithms (CCIA)</title>
      <link>https://binnz.github.io/post/2017-09-01-cluster-centroid-init/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-09-01-cluster-centroid-init/</guid>
      <description>&lt;p&gt;In iterative clustering algorithms, the procedure adopted for choosing initial cluster centers is extremely important as it has a direct impact on the formation of final clusters.   &lt;strong&gt;It is dangerous to select outliers as initial centers, since they are away from normal samples.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Cluster Center Initialization Algorithms (CCIA) is a density-based multi-scale data condensation.   This procedure is applicable to clustering algorithms for continuous data.   In CCIA, we assume that an individual attribute may provide some information about initial cluster center.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Data Mining - Basic Cluster Analysis</title>
      <link>https://binnz.github.io/post/2017-09-01-clustering/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-09-01-clustering/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;‚ÄúThe validation of clustering structures is the most difficult and frustrating part of cluster analysis.
Without a strong effort in this direction, cluster analysis will remain a black art accessible only to those true believers who have experience and great courage.‚Äù&lt;/p&gt;
&lt;p&gt;&amp;ndash; &lt;em&gt;Algorithms for Clustering Data&lt;/em&gt;, Jain and Dubes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Clustering Analysis is finding groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Intra-cluster distances are minimized&lt;/li&gt;
&lt;li&gt;Inter-cluster distances are maximized&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Divisive Method for Hierarchical Clustering and Minimum Spanning Tree Clustering</title>
      <link>https://binnz.github.io/post/2017-09-01-divisive-clustering/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-09-01-divisive-clustering/</guid>
      <description>&lt;p&gt;Divisive clustering &lt;strong&gt;starts with one, all-inclusive cluster&lt;/strong&gt;.   At each step, it &lt;strong&gt;splits a cluster until each cluster contains a point&lt;/strong&gt; (or there are k clusters).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Imbalanced Data Classification</title>
      <link>https://binnz.github.io/post/2017-07-25-imbalanced-data-classification/</link>
      <pubDate>Tue, 25 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-07-25-imbalanced-data-classification/</guid>
      <description>&lt;p&gt;Most of data in the real-word are imbalance in nature. Imbalanced class distribution is a scenario where the number of observations belonging to one class is significantly lower than those belonging to the other classes.   This happens because Machine Learning Algorithms are usually designed to improve accuracy by reducing the error. Thus, they do not take into account the class distribution / proportion or balance of classes.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Accuracy Paradox&lt;/strong&gt;
Accuracy Paradox is the case where your accuracy measures tell the story that you have excellent accuracy (such as 90%), but the accuracy is only reflecting the underlying class distribution.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Association Analysis of Sequence Data</title>
      <link>https://binnz.github.io/post/2017-07-01-sequence-data/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-07-01-sequence-data/</guid>
      <description>&lt;p&gt;A sequence is an ordered list of elements (transactions).   For example, purchase history of a given customer, history of events generated by a given sensor, browsing activity of a particular Web visitor, and so on.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ozzU8p0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;A sequence $s$ is defined as&lt;/p&gt;
&lt;p&gt;$$
s = &amp;lt;e_1~e_2~e_3~&amp;hellip;&amp;gt;
$$&lt;/p&gt;
&lt;p&gt;Where $e_i$ is the $i^{th}$ element, containing a collection of events (items) and attributed to a specific time or location.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Data Mining - Preprocessing for Association Analysis</title>
      <link>https://binnz.github.io/post/2017-07-01-preprocessing-association-analysis/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-07-01-preprocessing-association-analysis/</guid>
      <description>&lt;p&gt;Before we do association analysis, we need to handle the following 2 issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Categorical Attributes&lt;/li&gt;
&lt;li&gt;Continuous Attributes&lt;/li&gt;
&lt;li&gt;Multi-Level Concept Hierarchy&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Handling Continuous Attributes with Discretization-based Methods</title>
      <link>https://binnz.github.io/post/2017-07-01-handling-continuous-attributes/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-07-01-handling-continuous-attributes/</guid>
      <description>&lt;p&gt;Size of the discretized intervals affect support &amp;amp; confidence.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If intervals too small
&lt;ul&gt;
&lt;li&gt;may not have enough support&lt;/li&gt;
&lt;li&gt;e.g. {Refund = No, (Income = 51,250)} $\rightarrow$ {Cheat = No}&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If intervals too large
&lt;ul&gt;
&lt;li&gt;may not have enough confidence&lt;/li&gt;
&lt;li&gt;e.g. {Refund = No, (0K $\leq$ Income $\leq$ 1B)} $\rightarrow$ {Cheat = No}&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When there is any numerical attribute, the problem is to discretize individual numerical attribute into interesting intervals.   Each interval is represented as a Boolean attribute.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Brief Introduction of Spark Usage</title>
      <link>https://binnz.github.io/post/2017-03-28-brief-intro-of-spark-usage/</link>
      <pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-28-brief-intro-of-spark-usage/</guid>
      <description>&lt;p&gt;Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in &lt;strong&gt;Java, Scala, Python and R&lt;/strong&gt;, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Data Mining - Association Analysis</title>
      <link>https://binnz.github.io/post/2017-03-25-data-mining-association-analysis/</link>
      <pubDate>Sat, 25 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-25-data-mining-association-analysis/</guid>
      <description>&lt;p&gt;Association analysis is useful for discovering interesting relationships hidden in large data sets.   The uncovered relationships can be represented in the form of &lt;strong&gt;association rules&lt;/strong&gt; or sets of frequent items.&lt;/p&gt;
&lt;p&gt;For example, given a table of market basket transactions&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;TID&lt;/th&gt;
&lt;th&gt;Items&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;{Bread, Milk}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;{Bread, Diapers, Beer, Eggs}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;{Milk, Diapers, Beer, Cola}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;{Bread, Milk, Diapers, Beer}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;{Bread, Milk, Diapers, Cola}&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The follwing rule can be extracted from the table:&lt;/p&gt;
&lt;p&gt;$$
{Milk, Diaper} \rightarrow {Beer}
$$&lt;/p&gt;
&lt;p&gt;A common strategy adopted by many association rule mining algorithms is to decompose the problem into 2 major subtasks:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Frequent Itemset Generation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Find all the itemsets that satisfy the &lt;em&gt;minsup&lt;/em&gt; threshold.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Rule Generation&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Frequent Itemset Generation Using Apriori Algorithm</title>
      <link>https://binnz.github.io/post/2017-03-25-apriori/</link>
      <pubDate>Sat, 25 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-25-apriori/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The Apriori Principle&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;If an itemset is frequent, then all of its subsets must also be frequent.
Conversely, if an subset is infrequent, then all of its supersets must be infrequent, too.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The key idea of the Apriori Principle is monotonicity. By the &lt;strong&gt;anti-monotone&lt;/strong&gt; property of support, we can perform &lt;strong&gt;support-based pruning&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\forall X,Y: (X \subset Y) \rightarrow s(X) \geq s(Y)
$$&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Frequent Itemset Generation Using FP-Growth</title>
      <link>https://binnz.github.io/post/2017-03-25-frequent-itemset-generation-using-fp-growth/</link>
      <pubDate>Sat, 25 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-25-frequent-itemset-generation-using-fp-growth/</guid>
      <description>&lt;p&gt;FP-Growth uses FP-tree (Frequent Pattern Tree), a compressed representation of the database.   Once an FP-tree has been constructed, it uses a recursive divide-and-conquer approach to mine the frequent itemsets.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Generate a Candidate Hash Tree</title>
      <link>https://binnz.github.io/post/2017-03-25-generate-a-candidate-hash-tree/</link>
      <pubDate>Sat, 25 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-25-generate-a-candidate-hash-tree/</guid>
      <description>&lt;p&gt;To generate a candidate hash tree, the followings are required.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hash function&lt;/li&gt;
&lt;li&gt;Max leaf size - if number of candidate itemsets exceeds max leaf size, split the node&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Bayes Classification</title>
      <link>https://binnz.github.io/post/2017-03-23-bayes-classification/</link>
      <pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-23-bayes-classification/</guid>
      <description>&lt;p&gt;Bayes classification is a probabilistic framework for solving classification problems.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Instance Based Classification</title>
      <link>https://binnz.github.io/post/2017-03-22-instance-based-classification/</link>
      <pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-22-instance-based-classification/</guid>
      <description>&lt;p&gt;Basic Idea of Instance-Based Classification:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Store the training records&lt;/li&gt;
&lt;li&gt;Use training records to predict the class label of unseen cases&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Rule Based Classification</title>
      <link>https://binnz.github.io/post/2017-03-21-rule-based-classification/</link>
      <pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-21-rule-based-classification/</guid>
      <description>&lt;p&gt;Rule-Based Classifier classify records by using a collection of ‚Äúif‚Ä¶then‚Ä¶‚Äù rules.&lt;/p&gt;
&lt;p&gt;$$
(Condition) \rightarrow Class~Label
$$&lt;/p&gt;
&lt;p&gt;$$
(Blood Type=Warm) \wedge (Lay Eggs=Yes) \rightarrow Birds
\&lt;br&gt;
(Taxable Income &amp;lt; 50K) \vee (Refund=Yes) \rightarrow Evade=No
$$&lt;/p&gt;</description>
    </item>
    
    <item>
      <title> Time Series Analysis and Models</title>
      <link>https://binnz.github.io/post/2017-03-20-time-series/</link>
      <pubDate>Mon, 20 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-20-time-series/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time series&lt;/strong&gt;: A time series is a series of data points indexed (or listed or graphed) in time order.   A sequence taken at successive equally spaced points in time (A sequence of discrete-time data).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time series analysis&lt;/strong&gt;: Methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time series forecasting&lt;/strong&gt;: The use of a model to predict future values based on previously observed values.   While regression analysis is often employed in such a way as to test theories that the current values of one or more independent time series affect the current value of another time series, Time series forecasting focuses on comparing values of a single time series or multiple dependent time series at different points in time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time series modeling&lt;/strong&gt;: It involves working on time (years, days, hours, minutes) based data, to derive hidden insights to make informed decision making.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Data Mining - classification</title>
      <link>https://binnz.github.io/post/2017-03-19-data-science-classification/</link>
      <pubDate>Sun, 19 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-19-data-science-classification/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Classification as the task of learning a &lt;strong&gt;target function&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;f&lt;/em&gt;&lt;/strong&gt; that maps each attribute set &lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt; to one of the predicted class labels &lt;strong&gt;&lt;em&gt;y&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classification Tasks&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predicting tumor cells as benign or malignant&lt;/li&gt;
&lt;li&gt;Classifying credit card transactions as legitimate or fraudulent&lt;/li&gt;
&lt;li&gt;Categorizing news stories as finance, weather, entertainment, sports, etc.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Data Mining - data exploration</title>
      <link>https://binnz.github.io/post/2017-03-19-data-science-data-exploration/</link>
      <pubDate>Sun, 19 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-19-data-science-data-exploration/</guid>
      <description>&lt;p&gt;Data: Collection of data objects and their attributes&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Types of attributes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nominal: distinctness&lt;/li&gt;
&lt;li&gt;Oridinal: distinctness &amp;amp; order&lt;/li&gt;
&lt;li&gt;Interval: distinctness, order &amp;amp; addition&lt;/li&gt;
&lt;li&gt;Ratio: distinctness, order, addition &amp;amp; multiplication&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Types of data sets&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Record: data matrix, documents, transactions&lt;/li&gt;
&lt;li&gt;Graph: web, chemical structures&lt;/li&gt;
&lt;li&gt;Ordered: spatial/temporal data, sequential data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Data quality problems&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;noise&lt;/li&gt;
&lt;li&gt;outliers&lt;/li&gt;
&lt;li&gt;missing values&lt;/li&gt;
&lt;li&gt;duplicate data&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Decision Tree</title>
      <link>https://binnz.github.io/post/2017-03-19-decision-tree/</link>
      <pubDate>Sun, 19 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-19-decision-tree/</guid>
      <description>&lt;p&gt;Decision Tree Based Classification has the following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inexpensive to construct&lt;/li&gt;
&lt;li&gt;Extremely fast at classifying unknown records&lt;/li&gt;
&lt;li&gt;Easy to interpret for small-sized trees&lt;/li&gt;
&lt;li&gt;Accuracy is comparable to other classification techniques for many simple data sets&lt;/li&gt;
&lt;li&gt;Unsuitable for Large Datasets because sorting continuous attributes at each node needs entire data to fit in memory.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
