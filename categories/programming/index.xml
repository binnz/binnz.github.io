<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Programming on California Dreaming</title>
    <link>https://binnz.github.io/categories/programming/</link>
    <description>Recent content in Programming on California Dreaming</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 23 Sep 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://binnz.github.io/categories/programming/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Classification Models in Machine Learning</title>
      <link>https://binnz.github.io/post/2019-09-23-classification-models-in-machine-learning/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2019-09-23-classification-models-in-machine-learning/</guid>
      <description>&lt;p&gt;Classification is well so common in the area of machine learning and &lt;a href=&#34;https://scikit-learn.org/stable/&#34;&gt;scikit-learn&lt;/a&gt; provides a comprehensive toolkit that can be easily used.
Here I will share some common classification models and how to apply them on a dataset using this good toolkit, while the classification process will cover&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;training and testing&lt;/li&gt;
&lt;li&gt;cross validation and grid serach process&lt;/li&gt;
&lt;li&gt;classification performace display&lt;/li&gt;
&lt;li&gt;plots of area under curves&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Spark MLlib Programming Practice with Airline Dataset</title>
      <link>https://binnz.github.io/post/2018-09-17-spark-mllib/</link>
      <pubDate>Mon, 17 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-09-17-spark-mllib/</guid>
      <description>&lt;p&gt;In this document, I will build a predictive framework for predicting whether each flight in &lt;code&gt;2006&lt;/code&gt; will be cancelled or not by using the data from &lt;code&gt;2000&lt;/code&gt; to &lt;code&gt;2005&lt;/code&gt; as training data.&lt;/p&gt;
&lt;p&gt;Items to be delivered in this document includes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Show the predictive framework you designed. What features do you extract? What algorithms do you use in the framework?&lt;/li&gt;
&lt;li&gt;Explain the validation method you use.&lt;/li&gt;
&lt;li&gt;Explain the evaluation metric you use and show the effectiveness of your framework (i.e., use confusion matrix)&lt;/li&gt;
&lt;li&gt;Show the validation results and give a summary of results.&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>Practice in R: Analysis on all individuals diagnosed with Type I diabetes</title>
      <link>https://binnz.github.io/post/2018-09-10-r3/</link>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-09-10-r3/</guid>
      <description>&lt;p&gt;The Wisconsin Diabetes Registry Study targeted all individuals $&amp;lt;30$ years of age diagnosed with Type I diabetes in southern Wisconsin, USA. Participants were requested to submit blood samples and were sent a questionnaire inquiring about hospitalizations and other events. The blood samples were used to determine glycosylated hemoglobin (GHb), and important indicator of glycemic control.&lt;/p&gt;
&lt;p&gt;The data set &lt;code&gt;diabetes.txt&lt;/code&gt;, which can be downloaded from &lt;a href=&#34;http://ghuang.stat.nctu.edu.tw/course/datasci17/files/data/diabetes.txt&#34;&gt;here&lt;/a&gt;, contains the data from the Wisconsin Diabetes Registry Study. The data items are:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Variable name&lt;/th&gt;
&lt;th&gt;meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ID&lt;/td&gt;
&lt;td&gt;an unique identification number&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HEALTH&lt;/td&gt;
&lt;td&gt;self-reported health status: 1=excellent, 2=good, 3=fair, 4=poor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BH&lt;/td&gt;
&lt;td&gt;dichotomized health status: 1=excellent health, 0=worse than excellent health&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GENDER&lt;/td&gt;
&lt;td&gt;sex code: 1=female, 0=male&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GHb&lt;/td&gt;
&lt;td&gt;overal mean glycosylated hemoglobin value in study&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AGE&lt;/td&gt;
&lt;td&gt;age at diagnosis (years)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;In the section &amp;ldquo;Relationship of GHb with age&amp;rdquo;, we are interested in the relationship of GHb with age at diagnosis and/or self-reported health status&lt;/li&gt;
&lt;li&gt;In the section &amp;ldquo;Modeling of dichotomized health status&amp;rdquo;, we use BH as the dependent variable (response variable) and use models such as logistic regression to show the explanationary power of other independent variables.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Practice in R: Insights on Poverty</title>
      <link>https://binnz.github.io/post/2018-09-10-poverty/</link>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-09-10-poverty/</guid>
      <description>&lt;p&gt;This practice is based on Hans Rosling talks &lt;a href=&#34;https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en&#34;&gt;New Insights on Poverty&lt;/a&gt; and &lt;a href=&#34;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&#34;&gt;The Best Stats You&amp;rsquo;ve Ever Seen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The assignment uses data to answer specific question about global health and economics. The data contradicts commonly held preconceived notions. For example, Hans Rosling starts his talk by asking: &amp;ldquo;for each of the six pairs of countries below, which country do you think had the highest child mortality in 2015?&amp;rdquo;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sri Lanka or Turkey&lt;/li&gt;
&lt;li&gt;Poland or South Korea&lt;/li&gt;
&lt;li&gt;Malaysia or Russia&lt;/li&gt;
&lt;li&gt;Pakistan or Vietnam&lt;/li&gt;
&lt;li&gt;Thailand or South Africa&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Most people get them wrong. Why is this? In part it is due to our preconceived notion that the world is divided into two groups: the
&lt;em&gt;Western world&lt;/em&gt; versus the &lt;em&gt;third world&lt;/em&gt;, characterized by &amp;ldquo;long life,small family&amp;rdquo; and &amp;ldquo;short life, large family&amp;rdquo; respectively. In this homework we will use data visualization to gain insights on this topic.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Practice in R: Monte Carlo Simulation &amp; Central Limit Theorem (CLT)</title>
      <link>https://binnz.github.io/post/2018-09-10-r2/</link>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-09-10-r2/</guid>
      <description>&lt;p&gt;This is a practice in R for realization of Monte Carlo Simulation, Central Limit Theorem (CLT), normal approximation, and so on.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Neural Network Architecture and Back-Propagation</title>
      <link>https://binnz.github.io/post/2018-08-19-nn-xor/</link>
      <pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-08-19-nn-xor/</guid>
      <description>&lt;p&gt;The Universal Approximation Theorem states that a 2-layer network can approximate any function, given a complex enough architecture.  That&amp;rsquo;s why we will create a neural network with two neurons in the hidden layer and we will later show how this can model the XOR function.&lt;/p&gt;
&lt;p&gt;In this experiment, we will need to understand and write a simple neural network with backpropagation for “XOR” using only &lt;code&gt;numpy&lt;/code&gt; and other python standard library.&lt;/p&gt;
&lt;p&gt;The code here will allow the user to specify any number of layers and neurons in each layer.  In addition, we are going to use the logistic function as the activity function for this network.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Analyze Airline On-time Performance Dataset</title>
      <link>https://binnz.github.io/post/2018-06-14-flight/</link>
      <pubDate>Thu, 14 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-06-14-flight/</guid>
      <description>&lt;p&gt;In this document, we will walk through the data exploration of &lt;a href=&#34;http://stat-computing.org/dataexpo/2009/&#34;&gt;Airline on-time performance dataset&lt;/a&gt; with &lt;a href=&#34;https://pig.apache.org/&#34;&gt;Apache Pig&lt;/a&gt; as our exploring tool.   We will analyze &lt;strong&gt;8 years&lt;/strong&gt; of flight data to answer the following 3 analytic questions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find the maximal delays (you should consider both ArrDelay and DepDelay) for each month of 2008.&lt;/li&gt;
&lt;li&gt;How many flights were delayed caused by weather between 2000 ~ 2005? Please show the counting for each year.&lt;/li&gt;
&lt;li&gt;List Top 5 airports which occur delays most in 2007. (Please show the IATA airport code)&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>Spark Programming Practice on Hadoop Platform</title>
      <link>https://binnz.github.io/post/2018-06-14-hadoop-spark/</link>
      <pubDate>Thu, 14 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-06-14-hadoop-spark/</guid>
      <description>&lt;p&gt;The goal of this document is to practice Spark programming on Hadoop platform with the following problems.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In the text file (&lt;code&gt;Youvegottofindwhatyoulove.txt&lt;/code&gt;), show the &lt;strong&gt;top 30 most frequent occurring words&lt;/strong&gt; and their &lt;strong&gt;average occurrences in a sentence&lt;/strong&gt;   According to the result, what are the characteristics of these words?&lt;/li&gt;
&lt;li&gt;Implement a program to calculate the &lt;strong&gt;average amount in credit card trip for different number of passengers&lt;/strong&gt; which are from one to four passengers in &lt;strong&gt;2017.09&lt;/strong&gt; NYC Yellow Taxi trip data. In NYC Taxi data, the &amp;ldquo;Passenger_count&amp;rdquo; is a driver-entered value. Explain also how you &lt;strong&gt;deal with the data loss issue&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;For each of the above task 1 and 2, &lt;strong&gt;compare the execution time on local worker and yarn cluster&lt;/strong&gt;. Also, give some discussions on your observation.&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>Analyze the NYC Taxi Data</title>
      <link>https://binnz.github.io/post/2018-05-14-nyc/</link>
      <pubDate>Mon, 14 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-05-14-nyc/</guid>
      <description>&lt;p&gt;In this document, I will walk through the analysis of New York City Taxi Data (with download link shown in Section II) using &lt;code&gt;Python&lt;/code&gt;.   &lt;strong&gt;6 months of &amp;ldquo;Yellow&amp;rdquo; label data&lt;/strong&gt; will be loaded and analyzed.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Association Rule Mining 的 Java 程式實作</title>
      <link>https://binnz.github.io/post/2018-03-25-association-implementation/</link>
      <pubDate>Sun, 25 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-03-25-association-implementation/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://chih-ling-hsu.github.io/2017/03/25/Data-Mining-Association-Analysis&#34;&gt;關聯規則探勘(Association Rule Mining)&lt;/a&gt;是資料探勘領域中很常用的一種探勘方式，其中&lt;code&gt;Apriori&lt;/code&gt;演算法和&lt;code&gt;FP-Growth&lt;/code&gt;演算法是最為有名的。在這篇文章中，我會介紹我在這兩個演算法上的實作以及實作成果的實驗數據。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Clustering on New York City Bike Dataset</title>
      <link>https://binnz.github.io/post/2018-01-02-clustering-python/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-01-02-clustering-python/</guid>
      <description>&lt;p&gt;Our   major   task  here  is   turn   data   into   different   clusters and   explain   what
the   cluster   means.    We will try spatial   clustering, temporal   clustering and the combination of both.&lt;/p&gt;
&lt;p&gt;For each method of clustering, we will&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;try   &lt;strong&gt;at   least   2   values for   each parameter&lt;/strong&gt; in every algorithm.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;explain&lt;/strong&gt; the clustering result.&lt;/li&gt;
&lt;li&gt;make   some &lt;strong&gt;observation&lt;/strong&gt; ,    &lt;strong&gt;compare&lt;/strong&gt;    different   method   and   parameters.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Mining Association Rules on New York City Bike Dataset</title>
      <link>https://binnz.github.io/post/2018-01-01-association-rule-mining/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2018-01-01-association-rule-mining/</guid>
      <description>&lt;p&gt;What   we   want   to   do here   is  to design 3 mining tasks with their definitions of transactions and   find   some   rules   behind them.&lt;/p&gt;
&lt;p&gt;For   each   task,   we   should&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Try &lt;strong&gt;at   least   two   discretization   methods&lt;/strong&gt; (divided   by 10, divided   by   20, &amp;hellip;)&lt;/li&gt;
&lt;li&gt;Try &lt;strong&gt;at   least two   algorithms&lt;/strong&gt; (Apriori, FP-growth, &amp;hellip;) to   find   association   rules.&lt;/li&gt;
&lt;li&gt;List the interesting  rules.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compare&lt;/strong&gt;   the   differences   between   them.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Data Preprocessing and Exploring the New York City Bike Dataset</title>
      <link>https://binnz.github.io/post/2017-12-20-data-preprocessing/</link>
      <pubDate>Wed, 20 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-12-20-data-preprocessing/</guid>
      <description>&lt;p&gt;In this report, I will   do   some   data   preprocessing and   then   get   some   basic information   about   the   dataset, &lt;a href=&#34;https://www.citibikenyc.com/system-data&#34;&gt;New   York   Citi   Bike   Trip   Histories&lt;/a&gt;,    via   tools.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Data Manipulation and Visualization Using Elasticsearch and Kibana</title>
      <link>https://binnz.github.io/post/2017-09-24-elasticsearch-kibana/</link>
      <pubDate>Sun, 24 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-09-24-elasticsearch-kibana/</guid>
      <description>&lt;p&gt;Elasticsearch is a &lt;strong&gt;distributed&lt;/strong&gt;, &lt;strong&gt;real-time&lt;/strong&gt;, search and analytics platform.&lt;/p&gt;
&lt;p&gt;Using a restful API, Elasticsearch saves data and indexes it automatically. It assigns types to fields and that way a search can be done smartly and quickly using filters and different queries.&lt;/p&gt;
&lt;p&gt;It’s uses &lt;strong&gt;JVM&lt;/strong&gt; in order to be as fast as possible. It distributes indexes in “shards” of data. It replicates shards in different nodes, so it’s distributed and clusters can function even if not all nodes are operational. Adding nodes is super easy and that’s what makes it so scalable.&lt;/p&gt;
&lt;p&gt;ES uses &lt;strong&gt;Lucene&lt;/strong&gt; to solve searches. This is quite an advantage with comparing with, for example, Django query strings. A &lt;strong&gt;restful API&lt;/strong&gt; call allows us to perform searches using json objects as parameters, making it much more flexible and giving each search parameter within the object a different weight, importance and or priority.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Connect to Elastic Cloud with R Client</title>
      <link>https://binnz.github.io/post/2017-09-20-elasticsearch-r/</link>
      <pubDate>Wed, 20 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-09-20-elasticsearch-r/</guid>
      <description>&lt;p&gt;&lt;code&gt;Elastic&lt;/code&gt; (part of the &lt;a href=&#34;http://ropensci.org/&#34;&gt;rOpenSci project&lt;/a&gt;) is a general purpose R interface to Elasticsearch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ropensci/elastic&#34;&gt;Github Page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ropensci.org/tutorials/elastic_tutorial.html&#34;&gt;Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Brief Introduction to Popular Data Mining Algorithms</title>
      <link>https://binnz.github.io/post/2017-08-30-python-datamining/</link>
      <pubDate>Wed, 30 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-08-30-python-datamining/</guid>
      <description>&lt;p&gt;In this article, I will introduce a regression algorithm, linear regression, classical classifiers such as decision trees, naïve Bayes, and support vector machine, and unsupervised clustering algorithms such as k-means, and reinforcement learning techniques, the cross-entropy method, to give only a small glimpse of the variety of machine learning techniques that exist, and we will end this list by introducing neural networks.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Code Example of a Neural Network for The Function XOR</title>
      <link>https://binnz.github.io/post/2017-08-30-nn-xor/</link>
      <pubDate>Wed, 30 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-08-30-nn-xor/</guid>
      <description>&lt;p&gt;It is a well-known fact, and something we have already mentioned, that 1-layer neural networks cannot predict the function XOR. 1-layer neural nets can only classify linearly separable sets, however, as we have seen, the Universal Approximation Theorem states that a 2-layer network can approximate any function, given a complex enough architecture.&lt;/p&gt;
&lt;p&gt;We will now create a neural network with two neurons in the hidden layer and we will show how this can model the XOR function. However, we will write code that will allow the reader to simply modify it to allow for any number of layers and neurons in each layer, so that the reader can try simulating different scenarios. We are also going to use the hyperbolic tangent as the activity function for this network. To train the network, we will implement the back-propagation algorithm discussed earlier.&lt;/p&gt;
&lt;p&gt;In addition, if you are interested in the mathemetical derivation of this implementation, please see my another post &lt;a href=&#34;../../../2018/08/19/NN-XOR&#34;&gt;&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Simple Examples with Spark Streaming</title>
      <link>https://binnz.github.io/post/2017-05-02-spark-streaming/</link>
      <pubDate>Tue, 02 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-05-02-spark-streaming/</guid>
      <description>&lt;p&gt;Types of queries one wants on answer on a data stream:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sampling data from a stream - Construct a random sample&lt;/li&gt;
&lt;li&gt;Queries over sliding windows - Number of items of type x in the last k elements of the stream&lt;/li&gt;
&lt;li&gt;Filtering a data stream - Select elements with property x from the stream&lt;/li&gt;
&lt;li&gt;Counting distinct elements - Number of distinct elements in the last k elements of the stream&lt;/li&gt;
&lt;li&gt;Estimating moments - Estimate average/std deviation of last k elements&lt;/li&gt;
&lt;li&gt;Finding frequent elements&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Create an Excel VBA Add-In</title>
      <link>https://binnz.github.io/post/2017-04-06-create-an-excel-vba-add-in/</link>
      <pubDate>Thu, 06 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-04-06-create-an-excel-vba-add-in/</guid>
      <description>&lt;p&gt;An Excel Add-In is a file (usually with an .xla or .xll extension) that Excel can load when it starts up. The file contains code (VBA in the case of an .xla Add-In) that adds additional functionality to Excel, usually in the form of new functions.&lt;/p&gt;
&lt;p&gt;Add-Ins provide an excellent way of increasing the power of Excel and they are the ideal vehicle for distributing your custom functions. Excel is shipped with a variety of Add-Ins ready for you to load and start using, and many third-party Add-Ins are available.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Unit Testing in Python</title>
      <link>https://binnz.github.io/post/2017-03-30-unit-testing-in-python/</link>
      <pubDate>Thu, 30 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-30-unit-testing-in-python/</guid>
      <description>&lt;p&gt;If you want to be able to change or rewrite your code and know you didn&amp;rsquo;t break anything, proper unit testing is imperative.&lt;/p&gt;
&lt;p&gt;The unittest test framework is python’s xUnit style framework.
It is a standard module that you already have if you’ve got python version 2.1 or greater.   The &lt;code&gt;unittest&lt;/code&gt; module used to be called PyUnit, due to it’s legacy as a xUnit style framework.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How to Change Schema of a Spark SQL DataFrame?</title>
      <link>https://binnz.github.io/post/2017-03-28-how-to-change-schema-of-a-spark-sql-dataframe/</link>
      <pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-28-how-to-change-schema-of-a-spark-sql-dataframe/</guid>
      <description>&lt;p&gt;For the reason that I want to insert &lt;code&gt;rows selected from a table&lt;/code&gt; (&lt;em&gt;df_rows&lt;/em&gt;) to another table, I need to make sure that&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;The schema of the rows selected are the same as the schema of the table
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Since the function &lt;code&gt;pyspark.sql.DataFrameWriter.insertInto&lt;/code&gt;, which inserts the content of the DataFrame to the specified table, requires that the schema of the class:DataFrame is the same as the schema of the table.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Spark SQL Using Python</title>
      <link>https://binnz.github.io/post/2017-03-28-spark-sql-using-python/</link>
      <pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-28-spark-sql-using-python/</guid>
      <description>&lt;p&gt;For SQL users, Spark SQL provides state-of-the-art SQL performance and maintains compatibility with Shark/Hive. In particular, like Shark, Spark SQL supports all existing Hive data formats, user-defined functions (UDF), and the Hive metastore. With features that will be introduced in Apache Spark 1.1.0, Spark SQL beats Shark in TPC-DS performance by almost an order of magnitude.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Data Preprocessing on Airline Data</title>
      <link>https://binnz.github.io/post/2017-03-21-data-preprocessing-on-airline-data/</link>
      <pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-21-data-preprocessing-on-airline-data/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Data Source and Variable Definition:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://stat-computing.org/dataexpo/2009/the-data.html&#34;&gt;Statistical Computing Statistical Graphics&lt;/a&gt;
We are going to use flight information for &lt;em&gt;2000&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Logistic Regression Deployment Using Java Spark</title>
      <link>https://binnz.github.io/post/2017-03-21-logistic-regression-deployment/</link>
      <pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-21-logistic-regression-deployment/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Machine Learning on Spark using Java</title>
      <link>https://binnz.github.io/post/2017-03-21-ml-on-spark-using-java/</link>
      <pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://binnz.github.io/post/2017-03-21-ml-on-spark-using-java/</guid>
      <description>&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://spark.apache.org/downloads.html&#34;&gt;Download Apache Spark&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;MLlib is a built-in library of Spark&lt;/li&gt;
&lt;li&gt;Spark supports Python, Scala, and Java&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/programming-guide.html&#34;&gt;Spark Programming Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/configuration.html&#34;&gt;Sark Configuration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples&#34;&gt;Spark Java Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/ml-guide.html&#34;&gt;MLlib Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mapr.com/blog/apache-spark-machine-learning-tutorial&#34;&gt;Apache Spark Machine Learning Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/api/java/index.html&#34;&gt;Spark Java API doc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.bigsynapse.com/sampling-large-datasets-using-spark&#34;&gt;Sampling Large Datasets using Spark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
